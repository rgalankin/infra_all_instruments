---
id: 20260210220600-03
title: "Ollama"
summary: >
  Локальный embedding runtime. v0.15.5, 1 модель (274 MB), localhost:11434.
  Используется Obsidian Smart Connections. LLM-модели удалены — inference перенесён на ISHosting US.
type: spec
status: active
tags: [platform/macos, ai/llm]
source: roman
ai_weight: high
created: 2026-02-10
updated: 2026-02-15
category: "AI → Runtime → Local Embedding"
version: "0.15.5"
criticality: medium
platform: macOS
cost: "free"
---
# Ollama

## Основная информация

- **Категория:** AI → Runtime → Local Embedding
- **Версия:** 0.15.5
- **Статус:** активно
- **Критичность:** средняя (только embedding для Obsidian)
- **Назначение:** локальный embedding-движок для Smart Connections (Obsidian)
- **Платформа:** macOS (x86_64)
- **Endpoint:** `http://localhost:11434`

## Установленные модели (1)

| Модель | Параметры | Размер | Карточка |
|--------|----------|:------:|---------|
| nomic-embed-text | 137M | 274 MB | [[infra_all_instruments/docs/neural-networks/cards/nomic-embed-text__20260210220400-05\|→]] |

**Общий размер на диске:** 274 MB

### Удалённые модели (2026-02-15)

LLM-модели удалены — inference перенесён на бесплатные модели ISHosting US.
Экономия: ~47 GB дискового пространства.

| Модель | Причина удаления |
|--------|-----------------|
| qwen3:30b (18 GB) | 5-10 мин/запрос на iMac, бесполезно |
| qwen3-coder:30b (18 GB) | аналогично |
| deepseek-r1:8b (5.2 GB) | доступна через ISHosting US |
| llama3.1:8b (4.9 GB) | доступна через ISHosting US |
| qwen2.5-coder:1.5b-base (986 MB) | autocomplete через удалённый API |

## Настройки

### Основные параметры
- Endpoint: `http://localhost:11434`
- Данные моделей: `~/.ollama/models/`
- Конфигурация: `~/.ollama/`

### API

```bash
# Список моделей
GET /api/tags

# Генерация текста
POST /api/generate  {"model": "qwen3:30b", "prompt": "..."}

# Чат
POST /api/chat  {"model": "qwen3:30b", "messages": [...]}

# Эмбеддинги
POST /api/embeddings  {"model": "nomic-embed-text", "prompt": "..."}
```

## Ограничения на iMac Pro

| Ресурс | Значение | Влияние |
|--------|---------|---------|
| RAM | 32 GB DDR4 ECC | Модели до ~18 GB |
| GPU | Vega 56 8GB HBM2 | Ограниченное Metal-ускорение |
| Процессор | Xeon W 3.2GHz 8-core | CPU inference для 30B: медленно |

- Модели 30B: 5-10 мин на запрос (CPU-bound)
- Модели 8B: секунды на запрос
- Одновременно только одна 30B модель в RAM

## Связи

### Инструменты, использующие Ollama

| Инструмент | Модели | Как подключён |
|------------|--------|--------------|
| Obsidian (Smart Connections) | nomic-embed-text | плагин smart-connections → localhost:11434 |

> **TODO:** Перенастроить Continue и Dify на удалённый API (ISHosting US).
> Obsidian плагины ollama/textgenerator — решить, нужны ли при наличии удалённого API.

### Зависимости
- Требует: минимум ресурсов (274 MB, embedding-only)
- Используется в: Obsidian Smart Connections

## Команды

```bash
ollama list                    # Список установленных моделей
ollama run qwen3:30b          # Интерактивный режим
ollama pull <model>            # Скачать модель
ollama rm <model>              # Удалить модель
ollama show <model>            # Информация о модели
ollama ps                      # Активные модели в RAM
```

## Links (internal)

- [[infra_all_instruments/registry__20260210220000-01|Registry]]
- [[infra_all_instruments/docs/tools/_index__20260210220000-04|Реестр инструментов]]
- [[infra_all_instruments/docs/neural-networks/_index__20260210220000-05|Нейросети]]
- [[infra_all_instruments/docs/neural-networks/by-runtime/ollama-local__20260210220400-19|Ollama Local]]
- [[infra_all_instruments/docs/hardware/imac-pro-2017__20260210220100-01|iMac Pro]]
- [[infra_all_instruments/docs/tools/continue__20260210220600-02|Continue]] — использует Ollama для chat, autocomplete, embeddings
- [[infra_all_instruments/docs/tools/obsidian__20260210220600-01|Obsidian]] — плагины ollama, textgenerator, smart-connections
- [[infra_all_instruments/docs/tools/dify__20260210220600-09|Dify]] — подключена к Ollama для LLM

## История изменений

- 2026-02-15: Удалены 5 LLM-моделей (~47 GB), оставлена только nomic-embed-text (274 MB). Inference перенесён на ISHosting US.
- 2026-02-10: Создана карточка (данные из `ollama list` + `ollama --version`)
