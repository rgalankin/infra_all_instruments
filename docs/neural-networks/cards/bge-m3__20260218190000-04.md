---
id: 20260218190000-04
title: "BGE-M3 (BAAI)"
summary: "Open-source мультиязычная embedding-модель с тройной функциональностью: dense, sparse, multi-vector retrieval"
type: spec
status: active
tags: [ai/embedding, ai/rag, provider/baai, ai/multilingual, ai/open-source]
source: ai-research
ai_weight: high
created: 2026-02-18
updated: 2026-02-18
model_family: "BGE (BAAI General Embedding)"
model_params: "568M"
provider: "BAAI (Beijing Academy of Artificial Intelligence)"
runtime: "ollama-local / huggingface / siliconflow"
installed: false
used_in: []
---
# BGE-M3 (BAAI)

## Основная информация

- **Семейство:** BGE (BAAI General Embedding) --- M3 (Multi-Functionality, Multi-Linguality, Multi-Granularity)
- **Параметры:** 568M
- **Архитектура:** XLM-RoBERTa (fine-tuned)
- **Провайдер:** BAAI (Beijing Academy of Artificial Intelligence)
- **Рантайм:** Ollama (локально), HuggingFace, SiliconFlow (API), DeepInfra
- **Тип:** Embedding (НЕ LLM)
- **Назначение:** Мультиязычный семантический поиск, RAG, гибридный поиск
- **Open-source:** Да (MIT лицензия --- полностью бесплатно для коммерческого использования!)
- **Можно запустить локально:** Да

## Спецификации

| Характеристика | Значение |
|---------------|---------|
| Размерность | 1024 |
| Макс. контекст (токенов) | 8192 |
| Языки | 100+ (включая русский) |
| MTEB (en) avg | ~63.0 |
| Размер модели | ~2.2 GB (FP16) / ~1.2 GB (Q8) |
| Лицензия | MIT (полностью свободная) |

## Оценки

| Критерий | Оценка | Комментарий |
|----------|:------:|-------------|
| Качество (английский) | 80/100 | Хорошее; уступает Jina v3 и Voyage, но для open-source отличное |
| Качество (русский) | 85/100 | Одна из лучших для русского среди open-source |
| Мультиязычность | 95/100 | 100+ языков, SOTA на MIRACL (мультиязычный retrieval) |
| Скорость (локально) | 7/10 | 568M параметров --- нормальная скорость на GPU |
| Стоимость | 10/10 | Полностью бесплатная (MIT лицензия) |
| Гибридный поиск | 10/10 | Единственная модель с dense + sparse + multi-vector одновременно |

## Ценообразование

| Вариант | Стоимость | Примечание |
|---------|-----------|-----------|
| Локально (Ollama) | $0 | Нужно оборудование с GPU или мощный CPU |
| SiliconFlow API | ~$0 (бесплатные кредиты) | $1 стартовых кредитов; BGE-M3 среди бесплатных моделей |
| DeepInfra API | ~$0.005/1M tok | Очень дешёво |
| HuggingFace TEI | $0 | Нужен свой сервер |

## Ключевые особенности

### Тройная функциональность (M3)

BGE-M3 уникальна тем, что одна модель выполняет три типа retrieval одновременно:

1. **Dense retrieval:** Обычные плотные эмбеддинги (1024-мерный вектор) --- как у всех моделей
2. **Sparse retrieval:** Разреженные эмбеддинги --- аналог BM25, но на нейросетевом уровне
3. **Multi-vector retrieval:** ColBERT-подобный подход --- вектор для каждого токена

Это позволяет строить **гибридный поиск** с одной моделью вместо трёх.

### SOTA на мультиязычных бенчмарках

- **MIRACL:** Лучшие результаты на мультиязычном retrieval бенчмарке
- **MKQA:** Лучшие результаты на cross-lingual бенчмарке
- **ruMTEB:** Одна из лучших open-source моделей для русского (на уровне multilingual-e5-large)

## Поддержка русского языка

- **Статус:** Русский --- один из 100+ языков с хорошей поддержкой
- **ruMTEB:** Показывает результаты на уровне или лучше multilingual-e5-large для задач retrieval и reranking
- **MIRACL (Russian):** Высокие показатели nDCG@10
- **Токенизация:** XLM-RoBERTa токенизатор --- эффективнее для кириллицы, чем GPT-токенизатор
- **Практика:** По отзывам разработчиков, отлично работает с юридическими и бизнес-текстами на русском

## Запуск через Ollama

```bash
# Установка
ollama pull bge-m3

# Использование
curl http://localhost:11434/api/embeddings -d '{
  "model": "bge-m3",
  "prompt": "Процедура банкротства физических лиц"
}'
```

## Запуск через Python (HuggingFace)

```python
from FlagEmbedding import BGEM3FlagModel

model = BGEM3FlagModel("BAAI/bge-m3", use_fp16=True)

# Dense + Sparse + ColBERT одновременно
output = model.encode(
    ["Процедура банкротства физических лиц"],
    return_dense=True,
    return_sparse=True,
    return_colbert_vecs=True
)
```

## Требования для локального запуска

| Ресурс | Минимум | Рекомендуемо |
|--------|---------|-------------|
| RAM | 4 GB | 8 GB |
| VRAM (GPU) | --- | 4 GB+ |
| Диск | ~1.2 GB (Q8) | ~2.2 GB (FP16) |
| CPU | Любой x86_64 | 4+ ядер |

На VPS Beget с 4--8 GB RAM может работать через Ollama на CPU (без GPU), но медленнее.

## Когда выбирать

- **Выбирать, если:** Нужна лучшая бесплатная модель для русского; хотите гибридный поиск (dense+sparse); нужна MIT лицензия для коммерческого использования; работаете с ограниченным бюджетом
- **НЕ выбирать, если:** Нужна максимальная скорость (nomic-embed-text быстрее); сервер очень слабый (<4 GB RAM); нужен контекст >8192 токенов

## Альтернативы

| Модель | Отличие |
|--------|---------|
| nomic-embed-text-v1.5 | В 4 раза меньше (137M), быстрее, но хуже русский и нет гибридного поиска |
| Jina Embeddings v3 | Чуть лучше качество, но cc-by-nc-4.0 лицензия (не для коммерции) |
| multilingual-e5-large | Схожее качество русского, MIT лицензия, но контекст всего 512 токенов |
| Qwen3-Embedding-8B | Лучшее качество, но 8B параметров --- нужна мощная GPU |

## Links (internal)

- [[infra_all_instruments/docs/neural-networks/by-type/embedding__20260210220400-10|Embedding модели (индекс)]]
- [[infra_all_instruments/docs/neural-networks/_index__20260210220000-05|Реестр нейросетей]]
- [[infra_all_instruments/docs/neural-networks/by-runtime/ollama-local__20260210220400-19|Рантайм: Ollama Local]]

## История

- **2026-02-18:** Создана карточка (ai-research)
